{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-19T00:03:34.032563Z",
     "start_time": "2024-02-19T00:03:26.716720Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgb/miniforge3/envs/nlu/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expert: distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
      "Loading expert: mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expert: lxyuan/distilbert-base-multilingual-cased-sentiments-student\n",
      "Using device: mps\n",
      "Begin fitting model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10061 [00:01<?, ?it/s]\u001B[A\n",
      "  0%|          | 0/1000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 315>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    305\u001B[0m em \u001B[38;5;241m=\u001B[39m ExpertMixture(\n\u001B[1;32m    306\u001B[0m     eta\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.00005\u001B[39m,  \u001B[38;5;66;03m# Low learning rate for effective fine-tuning.\u001B[39;00m\n\u001B[1;32m    307\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m,  \u001B[38;5;66;03m# Small batches to avoid memory overload.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    312\u001B[0m     n_iter_no_change\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m,\n\u001B[1;32m    313\u001B[0m )\n\u001B[1;32m    314\u001B[0m dynasent_r1 \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdynabench/dynasent\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdynabench.dynasent.r1.all\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 315\u001B[0m \u001B[43mem\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdynasent_r1\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36mExpertMixture.fit\u001B[0;34m(self, dataset, *args, **kwargs)\u001B[0m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;66;03m# todo: fix mps memory issue\u001B[39;00m\n\u001B[1;32m    241\u001B[0m batch_preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\u001B[38;5;241m*\u001B[39minputs)\n\u001B[0;32m--> 243\u001B[0m err \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_preds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    247\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss\u001B[38;5;241m.\u001B[39mreduction \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    248\u001B[0m ):\n\u001B[1;32m    249\u001B[0m     err \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n",
      "File \u001B[0;32m~/miniforge3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/loss.py:1179\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1180\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1181\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/nlu/lib/python3.9/site-packages/torch/nn/functional.py:3053\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   3051\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3052\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3053\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "# PLEASE MAKE SURE TO INCLUDE THE FOLLOWING BETWEEN THE START AND STOP COMMENTS:\n",
    "#   1) Textual description of your system.\n",
    "#   2) The code for your original system.\n",
    "# PLEASE MAKE SURE NOT TO DELETE OR EDIT THE START AND STOP COMMENTS\n",
    "\n",
    "# START COMMENT: Enter your system description in this cell.\n",
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import dataclasses\n",
    "from datetime import datetime\n",
    "from typing import List, Type, Set, Dict\n",
    "\n",
    "import torch\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    HAS_WRITER = True\n",
    "except:\n",
    "    HAS_WRITER = False\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BatchEncoding\n",
    "\n",
    "import utils\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "\n",
    "DEFAULT_EXPERTS = [\n",
    "    \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n",
    "    \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
    "]\n",
    "\n",
    "EXPERT_SEP = -1\n",
    "\n",
    "\n",
    "if HAS_WRITER:\n",
    "    LOG = SummaryWriter(f\"./runs/{datetime.now().strftime('%Y_%m_%d-%H_%M')}\")\n",
    "\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class ExpertConfig:\n",
    "    n_classes: int = 3\n",
    "    expert_hidden_activation: Type[nn.Module] = nn.ReLU\n",
    "    arbiter_hidden_activation: Type[nn.Module] = nn.ReLU\n",
    "    arbiter_hidden_dim: int = 128\n",
    "    max_seq_length: int = 512\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        config: ExpertConfig,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        print(f\"Loading expert: {name}\")\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.name = name\n",
    "        self.tok = AutoTokenizer.from_pretrained(self.name)\n",
    "        self.model = AutoModel.from_pretrained(self.name)\n",
    "        self.hidden_dim = self.model.embeddings.word_embeddings.embedding_dim\n",
    "        self.hidden_activation = self.config.expert_hidden_activation()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            self.hidden_activation,\n",
    "            nn.Linear(self.hidden_dim, self.config.n_classes),\n",
    "        )\n",
    "\n",
    "    def encode_batch(self, examples: List[str]) -> torch.Tensor:\n",
    "        toks = self.tok.batch_encode_plus(\n",
    "            examples,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.config.max_seq_length,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        return toks\n",
    "\n",
    "    def decode_batch(self, examples: torch.Tensor) -> list[str]:\n",
    "        dec = self.tok.batch_decode(examples)\n",
    "        return dec\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        indices: torch.LongTensor,\n",
    "        mask: torch.LongTensor,\n",
    "    ) -> torch.LongTensor:\n",
    "        model_out = self.model(indices, attention_mask=mask, output_hidden_states=True)\n",
    "        last_hidden = model_out.last_hidden_state\n",
    "        preds = self.classifier(last_hidden[:, 0, :])\n",
    "        return preds\n",
    "\n",
    "\n",
    "class ExpertLayerWithArbiter(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        experts: list[str] = None,\n",
    "        config: ExpertConfig = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        experts = experts or DEFAULT_EXPERTS\n",
    "        self.config = config or ExpertConfig()\n",
    "        self.experts = nn.ModuleList([Expert(name, config) for name in experts])\n",
    "        self.arbiter = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                self.config.n_classes * len(self.experts),\n",
    "                self.config.arbiter_hidden_dim,\n",
    "            ),\n",
    "            self.config.arbiter_hidden_activation(),\n",
    "            nn.Linear(\n",
    "                self.config.arbiter_hidden_dim,\n",
    "                self.config.n_classes,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, masks: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        sep_idxs = [0] + torch.where(inputs[0] == EXPERT_SEP)[0].tolist()\n",
    "        assert len(sep_idxs) - 1 == len(self.experts), ValueError(\n",
    "            f\"Mismatch in number of separator indices ({len(sep_idxs)-1}) and number of experts ({len(self.experts)})!\"\n",
    "        )\n",
    "\n",
    "        outs = []\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            start = (\n",
    "                sep_idxs[expert_idx] if expert_idx == 0 else sep_idxs[expert_idx] + 1\n",
    "            )\n",
    "            stop = sep_idxs[expert_idx + 1]\n",
    "            expert_input = inputs[:, start:stop]\n",
    "            expert_mask = masks[:, start:stop]\n",
    "            expert_out = expert(expert_input.long(), expert_mask.long())\n",
    "            outs.append(expert_out)\n",
    "\n",
    "        # meow!\n",
    "        cat = torch.cat(outs, dim=1)\n",
    "\n",
    "        final = self.arbiter(cat)\n",
    "\n",
    "        return final\n",
    "\n",
    "\n",
    "class ExpertMixture(TorchShallowNeuralClassifier):\n",
    "\n",
    "    model: ExpertLayerWithArbiter\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        experts: list[str] | None = None,\n",
    "        config: ExpertConfig | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.experts = experts or DEFAULT_EXPERTS\n",
    "        self.config = config or ExpertConfig()\n",
    "\n",
    "        # set by .build_dataset()\n",
    "        self.classes_: Set[str] | None = None\n",
    "        self.n_classes_: int | None = None\n",
    "        self.class2index: dict[str, int] | None = None\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build_graph(self):\n",
    "        return ExpertLayerWithArbiter(self.experts, self.config)\n",
    "\n",
    "    def transform_for_experts(self, batch: Dict[str, list[str]]):\n",
    "        encoded = []\n",
    "        masks = []\n",
    "        x_batch = batch[\"sentence\"]\n",
    "        for expert in self.model.experts:\n",
    "            enc: BatchEncoding = expert.encode_batch(x_batch)\n",
    "            ids = enc[\"input_ids\"]\n",
    "            mask = enc[\"attention_mask\"]\n",
    "            batch_size = ids.shape[0]\n",
    "            demarcator = (torch.zeros(batch_size) - 1).unsqueeze(1)\n",
    "\n",
    "            # demarcate encodings and masks with -1\n",
    "            # since -1 never appears in any encodings\n",
    "            ids = torch.cat([ids, demarcator.clone()], dim=1)\n",
    "            mask = torch.cat([mask, demarcator.clone()], dim=1)\n",
    "\n",
    "            encoded.append(ids)\n",
    "            masks.append(mask)\n",
    "\n",
    "        y_batch = None\n",
    "        if \"gold_label\" in batch:\n",
    "            y_batch = batch[\"gold_label\"]\n",
    "            y_batch = [self.class2index[label] for label in y_batch]\n",
    "            y_batch = torch.tensor(y_batch)\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.cat(encoded, dim=1),\n",
    "            \"masks\": torch.cat(masks, dim=1),\n",
    "            \"labels\": y_batch,\n",
    "        }\n",
    "\n",
    "    def build_dataset(self, X: DatasetDict, subset: str = \"train\", *args, **kwargs):\n",
    "        x_subset = X[subset]\n",
    "\n",
    "        if not self.classes_:\n",
    "            y = x_subset[\"gold_label\"]\n",
    "            self.classes_ = sorted(set(y))\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            self.class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "\n",
    "        x_subset.set_transform(self.transform_for_experts)\n",
    "\n",
    "        return x_subset\n",
    "\n",
    "    def fit(self, dataset: DatasetDict, *args, **kwargs):\n",
    "        # Set up parameters needed to use the model. This is a separate\n",
    "        # function to support using pretrained models for prediction,\n",
    "        # where it might not be desirable to call `fit`.\n",
    "        self.initialize()\n",
    "\n",
    "        # TODO: delete this!\n",
    "        # self.device = \"cpu\" if self.device == \"mps\" else self.device\n",
    "\n",
    "        # Make sure the model is where we want it:\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Dataset:\n",
    "        train = self.build_dataset(dataset, \"train\")\n",
    "        dataloader = self._build_dataloader(train, shuffle=self.shuffle_train)\n",
    "        dev = []\n",
    "        if self.early_stopping:\n",
    "            dev = self.build_dataset(dataset, \"validation\")\n",
    "            dev = self._build_dataloader(dev)\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        print(\"Begin fitting model\")\n",
    "        for iteration in range(1, self.max_iter + 1):\n",
    "\n",
    "            epoch_error = 0.0\n",
    "\n",
    "            for batch_num, batch in tqdm(\n",
    "                enumerate(dataloader, start=1), total=len(dataloader)\n",
    "            ):\n",
    "\n",
    "                x_batch = batch[\"ids\"].to(self.device), batch[\"masks\"].to(self.device)\n",
    "                y_batch = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                batch_preds = self.model(*x_batch)\n",
    "\n",
    "                err = self.loss(batch_preds, y_batch)\n",
    "\n",
    "                if (\n",
    "                    self.gradient_accumulation_steps > 1\n",
    "                    and self.loss.reduction == \"mean\"\n",
    "                ):\n",
    "                    err /= self.gradient_accumulation_steps\n",
    "\n",
    "                err.backward()\n",
    "                if HAS_WRITER:\n",
    "                    LOG.add_scalar(\"Loss/train\", err.item(), batch_num)\n",
    "\n",
    "                epoch_error += err.item()\n",
    "\n",
    "                if (\n",
    "                    batch_num % self.gradient_accumulation_steps == 0\n",
    "                    or batch_num == len(dataloader)\n",
    "                ):\n",
    "                    if self.max_grad_norm is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.model.parameters(), self.max_grad_norm\n",
    "                        )\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "            # Stopping criteria:\n",
    "\n",
    "            if self.early_stopping and dev:\n",
    "                self._update_no_improvement_count_early_stopping(dev)\n",
    "                if self.no_improvement_count > self.n_iter_no_change:\n",
    "                    utils.progress_bar(\n",
    "                        \"Stopping after epoch {}. Validation score did \"\n",
    "                        \"not improve by tol={} for more than {} epochs. \"\n",
    "                        \"Final error is {}\".format(\n",
    "                            iteration, self.tol, self.n_iter_no_change, epoch_error\n",
    "                        ),\n",
    "                        verbose=self.display_progress,\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                self._update_no_improvement_count_errors(epoch_error)\n",
    "                if self.no_improvement_count > self.n_iter_no_change:\n",
    "                    utils.progress_bar(\n",
    "                        \"Stopping after epoch {}. Training loss did \"\n",
    "                        \"not improve more than tol={}. Final error \"\n",
    "                        \"is {}.\".format(iteration, self.tol, epoch_error),\n",
    "                        verbose=self.display_progress,\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "            utils.progress_bar(\n",
    "                \"Finished epoch {} of {}; error is {}\".format(\n",
    "                    iteration, self.max_iter, epoch_error\n",
    "                ),\n",
    "                verbose=self.display_progress,\n",
    "            )\n",
    "\n",
    "        if self.early_stopping:\n",
    "            self.model.load_state_dict(self.best_parameters)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _predict(self, dataloader: DataLoader):\n",
    "        self.model.eval()\n",
    "        preds = []\n",
    "        y = []\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "                x_batch = batch[\"ids\"].to(self.device), batch[\"masks\"].to(self.device)\n",
    "                y_batch = batch[\"labels\"].to(self.device)\n",
    "                preds.append(self.model(*x_batch))\n",
    "                y.append(y_batch)\n",
    "        return torch.cat(preds, axis=0), torch.cat(y, axis=0)\n",
    "\n",
    "    def _update_no_improvement_count_early_stopping(\n",
    "        self, dev_loader: DataLoader, *args\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Internal method used by `fit` to control early stopping.\n",
    "        The method uses `self.score(*dev)` for scoring and updates\n",
    "        `self.validation_scores`, `self.no_improvement_count`,\n",
    "        `self.best_score`, `self.best_parameters` as appropriate.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        preds, y = self._predict(dev_loader)\n",
    "        y = y.cpu().detach().numpy()\n",
    "        probs = torch.softmax(preds, dim=1).cpu().numpy()\n",
    "        preds = probs.argmax(axis=1)\n",
    "\n",
    "        score = utils.safe_macro_f1(y, preds)\n",
    "        self.validation_scores.append(score)\n",
    "        # If the score isn't at least `self.tol` better, increment:\n",
    "        if score < (self.best_score + self.tol):\n",
    "            self.no_improvement_count += 1\n",
    "        else:\n",
    "            self.no_improvement_count = 0\n",
    "        # If the current score is numerically better than all previous\n",
    "        # scores, update the best parameters:\n",
    "        if score > self.best_score:\n",
    "            self.best_parameters = copy.deepcopy(self.model.state_dict())\n",
    "            self.best_score = score\n",
    "        self.model.train()\n",
    "\n",
    "\n",
    "em = ExpertMixture(\n",
    "    eta=0.00005,  # Low learning rate for effective fine-tuning.\n",
    "    batch_size=8,  # Small batches to avoid memory overload.\n",
    "    gradient_accumulation_steps=4,  # Increase the effective batch size to 32.\n",
    "    early_stopping=True,  # Early-stopping\n",
    "    n_iter_no_change=5,\n",
    ")\n",
    "dynasent_r1 = load_dataset(\"dynabench/dynasent\", \"dynabench.dynasent.r1.all\")\n",
    "\n",
    "\n",
    "# STOP COMMENT: Please do not remove this comment.\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def checkpoint_model(net, optimizer, train_loss, val_loss, path):\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss\n",
    "            }, path)\n",
    "\n",
    "checkpoint_model(em, "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc7c4f2041f9b767"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f01b4399028798ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "em.fit(dynasent_r1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1bec8512b0fe0d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
