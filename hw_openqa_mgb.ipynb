{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf85bea0-68bf-4405-96ec-37579b2e9587",
   "metadata": {},
   "source": [
    "# Homework and bakeoff: Few-shot OpenQA with DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28e9bf5-7956-4c63-9129-7f2cbc468075",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-24T17:11:42.691648Z",
     "start_time": "2024-02-24T17:11:42.676460Z"
    }
   },
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts and Omar Khattab\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2024\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b5d964-a45c-496a-bb46-8f31d7b2d591",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cgpotts/cs224u/blob/master/hw_openqa.ipynb)\n",
    "[![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/cgpotts/cs224u/blob/master/hw_openqa.ipynb)\n",
    "\n",
    "If Colab is opened with this badge, please **save a copy to drive** (from the File menu) before running the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8570fc5-2ac0-4c0e-b350-71990937ebd8",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da2d82-8c54-4d41-a59d-891f83f85f6e",
   "metadata": {},
   "source": [
    "The goal of this homework is to explore retrieval-augmented in-context learning. This is an exciting area that brings together a number of recent task ideas and modeling innovations. We will use the [DSPy programming library](http://dspy.ai) to build systems in this new mode.\n",
    "\n",
    "Our core task is __open-domain question answering (OpenQA)__. In this task, all that is given by the dataset is a question text, and the task is to answer that question. By contrast, in many modern QA tasks, the dataset provides a text and a gold passage, usually with a firm guarantee that the answer will be a substring of the passage.\n",
    "\n",
    "OpenQA is substantially harder than standard QA. The usual strategy is to use a _retriever_ to find passages in a large collection of texts and train a _reader_ to find answers in those passages. This means we have no guarantee that the retrieved passage will contain the answer we need. If we don't retrieve a passage containing the answer, our reader has no hope of succeeding. Although this is challenging, it is much more realistic and widely applicable than standard QA. After all, with the right retriever, an OpenQA system could be deployed over the entire Web.\n",
    "\n",
    "The task posed by this homework is harder even than OpenQA. We are calling this task __few-shot OpenQA__. The defining feature of this task is that the reader is simply a frozen, general purpose language model. It accepts string inputs (prompts) and produces text in response. It is not trained to answer questions per se, and nothing about its structure ensures that it will respond with a substring of the prompt corresponding to anything like an answer.\n",
    "\n",
    "__Few-shot QA__ (but not OpenQA!) is explored in the famous GPT-3 paper ([Brown et al. 2020](https://arxiv.org/abs/2005.14165)). The authors are able to get traction on the problem using GPT-3, an incredible finding. Our task here – __few-shot OpenQA__ – pushes this even further by retrieving passages to use in the prompt rather than assuming that the gold passage can be used in the prompt. If we can make this work, then it should be a major step towards flexibly and easily deploying QA technologies in new domains.\n",
    "\n",
    "In summary:\n",
    "\n",
    "| Task             | Passage given | Task-specific reader training |Task-specific retriever training  | \n",
    "|-----------------:|:-------------:|:-----------------------------:|:--------------------------------:|\n",
    "| QA               | yes           | yes                           | n/a                              |\n",
    "| OpenQA           | no            | yes                           | maybe                            |\n",
    "| Few-shot QA      | yes           | no                            | n/a                              |\n",
    "| Few-shot OpenQA  | no            | no                            | maybe                            | \n",
    "\n",
    "Just to repeat: your mission is to explore the final line in this table. The core notebook and assignment don't address the issue of training the retriever in a task-specific way, but this is something you could pursue for a final project; [the ColBERT codebase](https://github.com/stanford-futuredata/ColBERT) makes easy.\n",
    "\n",
    "It is a requirement of the bake-off that a general-purpose language model be used. In particular, trained QA systems cannot be used at all, and no fine-tuning is allowed either. See the original system question at the bottom of this message for guidance on which models are allowed.\n",
    "\n",
    "Note: the models we are working with here are _big_. This poses a challenge that is increasingly common in NLP: you have to pay one way or another. You can pay to use the GPT-3 API, or you can pay to use a local model on a heavy-duty cluster computer, or you can pay with time by using a local model on a more modest computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd32bb4-067f-4cd6-943f-3e5574400beb",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149bcb0f-bc76-4277-a359-742d6dcee063",
   "metadata": {},
   "source": [
    "We have sought to make this notebook self-contained and easy to use on a personal computer, on Google Colab, and in Sagemaker Studio. For personal computer use, we assume you have already done everything in [setup.ipynb](setup.ipynb]). For cloud usage, the next few code blocks should handle all set-up steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b983bb-a20a-4c2a-9eee-9c553dd8c070",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:11:44.465147Z",
     "start_time": "2024-02-24T17:11:42.690161Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # This library is our indicator that the required installs\n",
    "    # need to be done.\n",
    "    import datasets\n",
    "    root_path = '.'\n",
    "except ModuleNotFoundError:\n",
    "    !git clone https://github.com/cgpotts/cs224u/\n",
    "    !pip install -r cs224u/requirements.txt\n",
    "    root_path = 'dspy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04cb488-cd40-4f9d-b884-8ff83b012042",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:11:44.722730Z",
     "start_time": "2024-02-24T17:11:44.465462Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import openai\n",
    "import os\n",
    "import dspy\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6298ae",
   "metadata": {},
   "source": [
    "Save the API keys in a `.env` file in the local root directory as follows. Then, `load_dotenv()` will make them available to the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c642de1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:11:44.729545Z",
     "start_time": "2024-02-24T17:11:44.722986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the API keys in a `.env` file in the local root directory\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9da704b-d27b-480a-93b5-e16cf7c51803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:11:44.741742Z",
     "start_time": "2024-02-24T17:11:44.729676Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(root_path, 'cache')\n",
    "\n",
    "openai_key = os.getenv('OPENAI_API_KEY')  # or replace with your API key (optional)\n",
    "\n",
    "colbert_server = 'http://index.contextual.ai:8893/api/search'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562fc500-bbab-48b7-a704-67c6d57bb09b",
   "metadata": {},
   "source": [
    "Here we establish the Language Model `lm` and Retriever Model `rm` that we will be using. The defaults for `lm` are just for development. You may want to develop using an inexpensive model and then do your final evalautions wih an expensive one. DSPy has support for a wide range of model APIs and local models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c118b014-e13f-433d-ad60-074636c7e738",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:11:44.744071Z",
     "start_time": "2024-02-24T17:11:44.732088Z"
    }
   },
   "outputs": [],
   "source": [
    "lm = dspy.OpenAI(model='gpt-3.5-turbo', api_key=openai_key)\n",
    "\n",
    "rm = dspy.ColBERTv2(url=colbert_server)\n",
    "\n",
    "dspy.settings.configure(lm=lm, rm=rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711785d7-6bb9-4041-92e6-cc5f9308477e",
   "metadata": {},
   "source": [
    "Here's a command you can run to see which OpenAI models are available; OpenAI has entered into an increasingly closed mode where many older models are not available, so there are likely to be some surprises lurking here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a859fbb-e985-4031-b8ed-34f3b034db8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:11:45.153931Z",
     "start_time": "2024-02-24T17:11:44.734599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['gpt-4-vision-preview',\n 'dall-e-3',\n 'gpt-3.5-turbo-0613',\n 'text-embedding-3-large',\n 'gpt-3.5-turbo-instruct-0914',\n 'dall-e-2',\n 'whisper-1',\n 'tts-1-hd-1106',\n 'tts-1-hd',\n 'gpt-3.5-turbo-16k',\n 'babbage-002',\n 'text-embedding-ada-002',\n 'gpt-3.5-turbo-0125',\n 'gpt-3.5-turbo',\n 'text-embedding-3-small',\n 'gpt-3.5-turbo-0301',\n 'gpt-3.5-turbo-instruct',\n 'gpt-4-1106-preview',\n 'tts-1',\n 'tts-1-1106',\n 'gpt-4-0125-preview',\n 'gpt-3.5-turbo-1106',\n 'gpt-4',\n 'gpt-4-turbo-preview',\n 'gpt-4-0613',\n 'gpt-3.5-turbo-16k-0613',\n 'davinci-002']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d.id for d in openai.models.list().data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b3dc2-87d7-4b8b-b603-ee567e008710",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de295d35-fea5-46d2-9a01-022ad88e54cd",
   "metadata": {},
   "source": [
    "Our core development dataset is [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/). We chose this dataset because it is well-known and widely used, and it is large enough to support lots of meaningful development work, without, though, being so large as to require lots of compute power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eaf2fd0-d060-4100-8702-f7311efd6129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:11:48.491142Z",
     "start_time": "2024-02-24T17:11:45.151724Z"
    }
   },
   "outputs": [],
   "source": [
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36965402-e3da-4531-b7e9-4b12cebcdf30",
   "metadata": {},
   "source": [
    "The following utility just reads a SQuAD split in as a list of `dspy.Example` instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21ad3e0b-7662-43b8-9409-a1a57442458b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:11:49.692818Z",
     "start_time": "2024-02-24T17:11:49.681627Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_squad_split(squad, split=\"validation\"):\n",
    "    \"\"\"\n",
    "    Use `split='train'` for the train split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dspy.Example with attributes question, answer\n",
    "\n",
    "    \"\"\"\n",
    "    data = zip(*[squad[split][field] for field in squad[split].features])\n",
    "    exs = [dspy.Example(question=q, answer=a['text'][0]).with_inputs(\"question\")\n",
    "           for eid, title, context, q, a in data]\n",
    "    return exs"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Example({'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answer': 'Saint Bernadette Soubirous'}) (input_keys={'question'}),\n Example({'question': 'What is in front of the Notre Dame Main Building?', 'answer': 'a copper statue of Christ'}) (input_keys={'question'}),\n Example({'question': 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?', 'answer': 'the Main Building'}) (input_keys={'question'}),\n Example({'question': 'What is the Grotto at Notre Dame?', 'answer': 'a Marian place of prayer and reflection'}) (input_keys={'question'})]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_squad_split(squad, split=\"train\")[:4]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-24T17:11:51.654330Z",
     "start_time": "2024-02-24T17:11:49.931407Z"
    }
   },
   "id": "99d730101980b2d9",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "a3847d38-4e70-46b7-bf46-4c8b784c5ee5",
   "metadata": {},
   "source": [
    "### SQuAD train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c91e1-586b-4747-be39-3092e60f182f",
   "metadata": {},
   "source": [
    "To build few-shot prompts, we will often sample SQuAD train examples, so we load that split here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c4feba-d580-4984-a449-0b92a53ef13a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:11:53.505234Z",
     "start_time": "2024-02-24T17:11:51.652955Z"
    }
   },
   "outputs": [],
   "source": [
    "squad_train = get_squad_split(squad, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab8c41-8eae-4d15-ad4d-e28b3c58eb4a",
   "metadata": {},
   "source": [
    "### SQuAD dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37198b33-c47b-4e0e-af8b-c00860658cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:11:53.644403Z",
     "start_time": "2024-02-24T17:11:53.580394Z"
    }
   },
   "outputs": [],
   "source": [
    "squad_dev = get_squad_split(squad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40c768-57cc-4a07-a3ef-5e34262b0ace",
   "metadata": {},
   "source": [
    "### SQuAD dev sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636601b5-c7ad-4177-a6d6-f3afdb0bedae",
   "metadata": {},
   "source": [
    "Evaluations are expensive in this new era! Here's a small sample to use for dev assessments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64ccdd0e-de78-440d-bfbe-358c12eada5c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-24T17:13:16.538379Z",
     "start_time": "2024-02-24T17:13:16.505788Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "dev_exs = random.sample(squad_dev, k=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28265d01-890d-4f04-b518-da0e8a1cb235",
   "metadata": {},
   "source": [
    "## DSPy basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b9d12-fc8c-4aae-b09e-0d72a4aa54f5",
   "metadata": {},
   "source": [
    "### LM usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c278daac-11f9-4327-a06f-1c408a06a71d",
   "metadata": {},
   "source": [
    "Here's the most basic way to use the LM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02364ed6-3c6d-4eaf-849a-2d9e30d84b53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:13:23.391060Z",
     "start_time": "2024-02-24T17:13:23.350842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Gary Zukav\\'s first book, \"The Dancing Wu Li Masters: An Overview of the New Physics,\" received the 1979 American Book Award for Science.']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"Which award did Gary Zukav's first book receive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2356d9a8-750b-4383-bc5b-4173ca5c13ac",
   "metadata": {},
   "source": [
    "Keyword arguments to the underlying LM are passed through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19ab8deb-3b9d-4f57-bd70-7c170be294c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:13:27.379042Z",
     "start_time": "2024-02-24T17:13:27.346354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Maine and Alaska are the only two U.S. states that do not share a border with any other U.S. states.',\n 'Maine and Vermont border no other U.S. states.',\n 'The U.S. states that do not border any other U.S. states are:\\n\\n1. Alaska\\n2. Hawaii',\n 'Hawaii, Alaska, and Florida do not share a border with any other U.S. states.']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"Which U.S. states border no U.S. states?\", temperature=0.9, n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50e8d99-6d49-420d-ab5d-cc01b53cd4a1",
   "metadata": {},
   "source": [
    "With `lm.inspect_history`, we can see the most recent language model calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f7cb5a5-3a3f-4e78-b9af-488fadc896ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-24T17:13:51.894851Z",
     "start_time": "2024-02-24T17:13:51.865278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Which U.S. states border no U.S. states?\u001B[32m Maine and Alaska are the only two U.S. states that do not share a border with any other U.S. states.\u001B[0m\u001B[31m \t (and 3 other completions)\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a0f2df-df1f-422d-bff3-6c4a3d947f6e",
   "metadata": {},
   "source": [
    "### Signature-based prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0888a2a-fcaa-44b0-beef-b097c856c74b",
   "metadata": {},
   "source": [
    "In DSPy, __signatures__ are declarative statements about what we want the model to do. In the following `\"question -> answer\"` is the signature (the most basic QA signature one could write), and `dspy.Predict` is used to turn this into a complete QA system: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11f04cba-7d46-4680-a154-a0062243618e",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-24T17:13:52.661645Z",
     "start_time": "2024-02-24T17:13:52.633262Z"
    }
   },
   "outputs": [],
   "source": [
    "basic_predictor = dspy.Predict(\"question -> answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994fc10-8c28-4836-bd7c-008a9a3d34e4",
   "metadata": {},
   "source": [
    "Here we use `basic_predictor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfe102d0-c0e5-45ad-aae8-51f6ea6e70f6",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-24T17:13:53.026248Z",
     "start_time": "2024-02-24T17:13:52.996096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Prediction(\n    answer='The Dancing Wu Li Masters received the American Book Award for Science.'\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_predictor(question=\"Which award did Gary Zukav's first book receive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad1a1b-d422-46df-8b03-4054bfec5fc1",
   "metadata": {},
   "source": [
    "And here is the prompt that was given to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "af4f86db-97d5-4742-8468-4627de12f181",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:03:04.769232Z",
     "start_time": "2024-02-25T18:03:04.732883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions with short factoid answers.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may contain relevant facts\n",
      "Question: ${question}\n",
      "Answer: often between 1 and 5 words\n",
      "\n",
      "---\n",
      "\n",
      "Question: What group did Paul VI address in New York in 1965?\n",
      "Answer: United Nations\n",
      "\n",
      "Question: What did Sander's study show in terms of black law students rankings?\n",
      "Answer: half of all black law students rank near the bottom of their class after the first year of law school\n",
      "\n",
      "Question: What problems does linguistic anthropology bring linguistic methods to bear on?\n",
      "Answer: anthropological\n",
      "\n",
      "Question: What did Locke, Hobbes, and Rousseau agree was necessary for a man to live in civil society?\n",
      "Answer: social contract\n",
      "\n",
      "Question: Aside from die casting, what are alloys of zinc mixed with copper, aluminium and magnesium used for?\n",
      "Answer: spin casting\n",
      "\n",
      "Question: Where was the initial mention of the Oeselians?\n",
      "Answer: Ptolemy's Geography III\n",
      "\n",
      "Question: In 2001, what percent of Nigeria's population was Muslim?\n",
      "Answer: about 50%\n",
      "\n",
      "Question: What was the fate of the proposal to make Taiwanese the second official language?\n",
      "Answer: the proposal did not pass\n",
      "\n",
      "Question: Under ancient custom, an Act of Tynwald didn't go into effect until decreed at which location?\n",
      "Answer: Tynwald Hill at St John's\n",
      "\n",
      "Question: London's government administration is comprised of how many tiers?\n",
      "Answer: two\n",
      "\n",
      "Question: What is database storage?\n",
      "Answer: physical materialization of a database\n",
      "\n",
      "Question: Where did \"Johnny Todd\"--the theme song for Everton matchdays--originate?\n",
      "Answer: Liverpool\n",
      "\n",
      "Question: Which group was predecessor to the Liberals?\n",
      "Answer: United Australia Party (UAP)\n",
      "\n",
      "Question: According to most linguistics, who speaks a dialect?\n",
      "Answer: everybody\n",
      "\n",
      "Question: Where did Hisham base his court?\n",
      "Answer: Resafa\n",
      "\n",
      "Question: Aside from Italo-Dalmatian, what is another term for the group that Italian languages belong to?\n",
      "Answer: Romance\n",
      "\n",
      "Context: «Basil Moreau | day of the Congregation of Holy Cross. Blessed Basil Moreau is commemorated on January 20. Education is Moreau's lasting legacy. Moreau Seminary, located on the University of Notre Dame campus, is the main seminary for the American congregation of the Holy Cross Fathers. Moreau is credited with playing a key role in the foundation of the University of Notre Dame as well as Saint Mary's College in Notre Dame, Indiana. He had a vision for unifying the two same-sex schools, but was prevented from doing so by the Vatican in Rome due to their historical beliefs that men and women»\n",
      "Question: What is the primary seminary of the Congregation of the Holy Cross?\n",
      "Answer:\u001B[32m Moreau Seminary\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845f62b-8af3-4fa2-af8d-f4ddd1181f30",
   "metadata": {},
   "source": [
    "In many cases, we will want more control over the prompt. Writing a small custom `dspy.Signature` class is the easiest way to accomplish this. In the following, we just just tweak the initial instruction and provide some formatting guidance for the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9dda770-e7ca-4682-b283-d4f4525adb68",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-24T17:13:54.725960Z",
     "start_time": "2024-02-24T17:13:54.717529Z"
    }
   },
   "outputs": [],
   "source": [
    "class BasicQASignature(dspy.Signature):\n",
    "    __doc__ = \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83f7297a-b174-4566-8748-6b66d515ed25",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-24T17:13:55.849741Z",
     "start_time": "2024-02-24T17:13:55.841159Z"
    }
   },
   "outputs": [],
   "source": [
    "sig_predictor = dspy.Predict(BasicQASignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30a02d55-fac7-43c3-851b-3e21f06df14d",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-24T17:13:55.973968Z",
     "start_time": "2024-02-24T17:13:55.962982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Prediction(\n    answer='Maine, Hawaii'\n)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_predictor(question=\"Which U.S. states border no U.S. states?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f613c05-ec78-4129-ac44-71bac4e253ad",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-24T17:13:57.247232Z",
     "start_time": "2024-02-24T17:13:57.218315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions with short factoid answers.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "Answer: often between 1 and 5 words\n",
      "\n",
      "---\n",
      "\n",
      "Question: Which U.S. states border no U.S. states?\n",
      "Answer:\u001B[32m Maine, Hawaii\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56169f83-0df6-46dc-b617-fadff1b96132",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e244aed0-403e-4707-a9eb-e29fc1e4dc57",
   "metadata": {},
   "source": [
    "One of the hallmarks of DSPy is that it adopts design patterns from PyTorch. The main example of this is DSPy's use of the `Module` as the basic unit for writing simple and complex programs. Here is a very basic module for QA that makes use of `BasicQASignature` as we defined it just above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "506c9946-fee4-4dc3-a05e-767f85c25292",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-24T17:14:03.375840Z",
     "start_time": "2024-02-24T17:14:03.346844Z"
    }
   },
   "outputs": [],
   "source": [
    "class BasicQA(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(BasicQASignature)\n",
    "\n",
    "    def forward(self, question):\n",
    "        return self.generate_answer(question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921aaed-6351-443d-8478-6e9a64b49d45",
   "metadata": {},
   "source": [
    "As with PyTorch, the `forward` method is called when we want to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d67f65b7-0fea-40a3-ac78-795926025653",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:50.503562Z",
     "start_time": "2024-02-25T18:08:50.474470Z"
    }
   },
   "outputs": [],
   "source": [
    "basic_qa_model = BasicQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7044ec84-f76a-4af4-93f5-a82579237868",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:50.750656Z",
     "start_time": "2024-02-25T18:08:50.724047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Prediction(\n    answer='National Book Award'\n)"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_qa_model(question=\"Which award did Gary Zukav's first book receive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9961c89-914a-4736-8a01-1a3cd401821a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The modular design of DSPy starts to become apparent now. If you want to change the above to use chain of thought instead of regular predictions, you need only change `dspy.Predict` to `dspy.ChainOfThought`, and similarly for `dspy.ReAct`, `dspy.ProgramOfThought`, or a module you wrote yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9dac4-9495-448d-ad4b-38ab7260915b",
   "metadata": {},
   "source": [
    "### Teleprompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5200696-808f-4061-878c-26aa20701d58",
   "metadata": {},
   "source": [
    "The QA system we've defined so far is a zero-shot system. To change it into a few-shot system, we will rely on a DSPy __teleprompter__. This will allow us to flexibly move between the zero-shot and few-shot formulations. The following code achieves this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fdab9850-1d4d-4e04-b389-6ef55fd0b425",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:53.082384Z",
     "start_time": "2024-02-25T18:08:53.073576Z"
    }
   },
   "outputs": [],
   "source": [
    "from dspy.teleprompt import LabeledFewShot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72045705-2f27-46fe-b7bc-801a7d2e9ae4",
   "metadata": {},
   "source": [
    "Here we instantiate a `LabeledFewShot` teleprompter that will add three demonstrations. These will be sampled randomly from the set of train examples we provide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "befc8c18-fc64-4947-800d-6455dee90b68",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:53.348240Z",
     "start_time": "2024-02-25T18:08:53.339059Z"
    }
   },
   "outputs": [],
   "source": [
    "fewshot_teleprompter = LabeledFewShot(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b512d9-ec54-49ca-ab7a-900dc2eea6cc",
   "metadata": {},
   "source": [
    "And then we call `compile` on `basic_qa_model` as we defined it above. This returns a new module that we use like any other in DSPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b327b603-2943-4bcc-b498-1fbafafefd0c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:53.654752Z",
     "start_time": "2024-02-25T18:08:53.646700Z"
    }
   },
   "outputs": [],
   "source": [
    "basic_fewshot_qa_model = fewshot_teleprompter.compile(basic_qa_model, trainset=squad_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "921ec7f1-ed44-48a3-9c92-0e22911688f0",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:53.843726Z",
     "start_time": "2024-02-25T18:08:53.815487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Prediction(\n    answer='American Book Award'\n)"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_fewshot_qa_model(question=\"Which award did Gary Zukav's first book receive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180cdaa-dd34-4e0d-8455-e394bfde9fcc",
   "metadata": {},
   "source": [
    "With `inspect_history`, we can see that prompts now contain demonstrations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f9507fe6-1cc7-4c0a-8520-cc3812a07c27",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:54.136209Z",
     "start_time": "2024-02-25T18:08:54.112949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions with short factoid answers.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Question: ${question}\n",
      "Answer: often between 1 and 5 words\n",
      "\n",
      "---\n",
      "\n",
      "Question: What group did Paul VI address in New York in 1965?\n",
      "Answer: United Nations\n",
      "\n",
      "---\n",
      "\n",
      "Question: What did Sander's study show in terms of black law students rankings?\n",
      "Answer: half of all black law students rank near the bottom of their class after the first year of law school\n",
      "\n",
      "---\n",
      "\n",
      "Question: What problems does linguistic anthropology bring linguistic methods to bear on?\n",
      "Answer: anthropological\n",
      "\n",
      "---\n",
      "\n",
      "Question: Which award did Gary Zukav's first book receive?\n",
      "Answer:\u001B[32m American Book Award\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba48440-4a65-41e8-b397-7b79f65fa0fe",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8734a-49a1-4093-a2fb-09bb7d2f2859",
   "metadata": {},
   "source": [
    "Our evaluation metric is a standard one for SQuAD and related tasks: exact match of the answer (EM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f052a79f-ad9f-4f3e-a195-809567e2eea1",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:55.689997Z",
     "start_time": "2024-02-25T18:08:55.659708Z"
    }
   },
   "outputs": [],
   "source": [
    "from dspy.evaluate import answer_exact_match\n",
    "from dspy.evaluate.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7341a846-5158-4acf-8aa5-aca61ef40174",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:55.819607Z",
     "start_time": "2024-02-25T18:08:55.795145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_exact_match(dspy.Example(answer=\"STAGE 2!\"), dspy.Prediction(answer=\"stage 2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f640e5bf-823f-4cdb-92da-a28f5cea7760",
   "metadata": {},
   "source": [
    "In DSPy, `Evaluate` objects provide a uniform interface for running evaluations. Here are two for us to use in development. The first will evaluate on all of `dev_exs` and should provide a meaningful picture of how a system is doing. It could be expensive to use it a lot, though. The second is for debugging and is probably too small to give a reliable estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e6a3b58b-c114-4316-b8d7-2d8049132c1d",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:56.522663Z",
     "start_time": "2024-02-25T18:08:56.514454Z"
    }
   },
   "outputs": [],
   "source": [
    "dev_evaluater = Evaluate(\n",
    "    devset=dev_exs, # 200 examples\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    "    display_table=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "72fd504e-4684-445d-b0cc-363cd1685b73",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:56.717778Z",
     "start_time": "2024-02-25T18:08:56.708257Z"
    }
   },
   "outputs": [],
   "source": [
    "tiny_evaluater = Evaluate(\n",
    "    devset=dev_exs[: 15],\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    "    display_table=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d88a6c-55b6-4994-9413-1a2301c94903",
   "metadata": {},
   "source": [
    "Here is a tiny (debugging-oriented) evaluation of our few-shot QA sytem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4f736065-39f2-4d76-a258-e852767dbb8e",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:57.044365Z",
     "start_time": "2024-02-25T18:08:56.988195Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 15  (13.3): 100%|██████████| 15/15 [00:00<00:00, 2717.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 15  (13.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/mgb/miniforge3/envs/nlu/lib/python3.9/site-packages/dspy/evaluate/evaluate.py:137: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x2b7d24820>",
      "text/html": "<style type=\"text/css\">\n#T_b97aa th {\n  text-align: left;\n}\n#T_b97aa td {\n  text-align: left;\n}\n#T_b97aa_row0_col0, #T_b97aa_row0_col1, #T_b97aa_row0_col2, #T_b97aa_row0_col3, #T_b97aa_row1_col0, #T_b97aa_row1_col1, #T_b97aa_row1_col2, #T_b97aa_row1_col3, #T_b97aa_row2_col0, #T_b97aa_row2_col1, #T_b97aa_row2_col2, #T_b97aa_row2_col3, #T_b97aa_row3_col0, #T_b97aa_row3_col1, #T_b97aa_row3_col2, #T_b97aa_row3_col3, #T_b97aa_row4_col0, #T_b97aa_row4_col1, #T_b97aa_row4_col2, #T_b97aa_row4_col3 {\n  text-align: left;\n  white-space: pre-wrap;\n  word-wrap: break-word;\n  max-width: 400px;\n}\n</style>\n<table id=\"T_b97aa\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_b97aa_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n      <th id=\"T_b97aa_level0_col1\" class=\"col_heading level0 col1\" >example_answer</th>\n      <th id=\"T_b97aa_level0_col2\" class=\"col_heading level0 col2\" >pred_answer</th>\n      <th id=\"T_b97aa_level0_col3\" class=\"col_heading level0 col3\" >answer_exact_match</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_b97aa_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_b97aa_row0_col0\" class=\"data row0 col0\" >In 1517 who was Luther's bishop?</td>\n      <td id=\"T_b97aa_row0_col1\" class=\"data row0 col1\" >Albert of Mainz</td>\n      <td id=\"T_b97aa_row0_col2\" class=\"data row0 col2\" >Albert of Mainz</td>\n      <td id=\"T_b97aa_row0_col3\" class=\"data row0 col3\" >✔️ [True]</td>\n    </tr>\n    <tr>\n      <th id=\"T_b97aa_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_b97aa_row1_col0\" class=\"data row1 col0\" >When was the construction that changed the Rhine's Delta?</td>\n      <td id=\"T_b97aa_row1_col1\" class=\"data row1 col1\" >20th Century</td>\n      <td id=\"T_b97aa_row1_col2\" class=\"data row1 col2\" >13th century</td>\n      <td id=\"T_b97aa_row1_col3\" class=\"data row1 col3\" >False</td>\n    </tr>\n    <tr>\n      <th id=\"T_b97aa_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_b97aa_row2_col0\" class=\"data row2 col0\" >How many companies were registered in Warsaw in 2006?</td>\n      <td id=\"T_b97aa_row2_col1\" class=\"data row2 col1\" >304,016</td>\n      <td id=\"T_b97aa_row2_col2\" class=\"data row2 col2\" >over 100,000</td>\n      <td id=\"T_b97aa_row2_col3\" class=\"data row2 col3\" >False</td>\n    </tr>\n    <tr>\n      <th id=\"T_b97aa_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_b97aa_row3_col0\" class=\"data row3 col0\" >What is the CJEU's duty?</td>\n      <td id=\"T_b97aa_row3_col1\" class=\"data row3 col1\" >to \"ensure that in the interpretation and application of the Treaties the law is observed\"</td>\n      <td id=\"T_b97aa_row3_col2\" class=\"data row3 col2\" >interpret EU law</td>\n      <td id=\"T_b97aa_row3_col3\" class=\"data row3 col3\" >False</td>\n    </tr>\n    <tr>\n      <th id=\"T_b97aa_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_b97aa_row4_col0\" class=\"data row4 col0\" >What would a teacher do for someone who is cocky?</td>\n      <td id=\"T_b97aa_row4_col1\" class=\"data row4 col1\" >deflate</td>\n      <td id=\"T_b97aa_row4_col2\" class=\"data row4 col2\" >challenge them</td>\n      <td id=\"T_b97aa_row4_col3\" class=\"data row4 col3\" >False</td>\n    </tr>\n  </tbody>\n</table>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                <div style='\n                    text-align: center; \n                    font-size: 16px; \n                    font-weight: bold; \n                    color: #555; \n                    margin: 10px 0;'>\n                    ... 10 more rows not displayed ...\n                </div>\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "13.33"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_evaluater(basic_fewshot_qa_model, metric=answer_exact_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1b41d-760c-4f28-8a2d-7f037b4f9d97",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d4c8f4-a537-4d9b-9500-f881fceef1de",
   "metadata": {},
   "source": [
    "The final major component of our systems is retrieval. When we defined `rm`, we connected to a remote ColBERT index and retriever system that we can now use for search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dfd114-96cf-468a-bac3-d3d39d6f3ca6",
   "metadata": {},
   "source": [
    "The basic `dspy.retrieve` method returns only passages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d8f4cafc-f5db-41c0-9561-ecde61331666",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:58.823875Z",
     "start_time": "2024-02-25T18:08:58.816623Z"
    }
   },
   "outputs": [],
   "source": [
    "retriever = dspy.Retrieve(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "891fc391-c177-4da7-9332-ab20cdba3c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:59.160304Z",
     "start_time": "2024-02-25T18:08:59.150337Z"
    }
   },
   "outputs": [],
   "source": [
    "passages = retriever(\"Which award did Gary Zukav's first book receive?\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-25T18:08:59.240001Z",
     "start_time": "2024-02-25T18:08:59.232495Z"
    }
   },
   "id": "963dba47edbe325e",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "abdac37b-b5fe-421c-826f-4fd699cb2e36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:00.159483Z",
     "start_time": "2024-02-25T18:09:00.149600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Prediction(\n    passages=['Gary Zukav | Gary Zukav Gary Zukav (born October 17, 1942) is an American spiritual teacher and the author of four consecutive New York Times Best Sellers. Beginning in 1998, he appeared more than 30 times on \"The Oprah Winfrey Show\" to discuss transformation in human consciousness concepts presented in his book \"The Seat of the Soul\". His first book, \"The Dancing Wu Li Masters\" (1979), won a U.S. National Book Award. Gary Zukav was born in Port Arthur, Texas, and spent his early childhood in San Antonio and Houston. His family moved to Pittsburg, Kansas, while he was in fourth grade. In', 'The Dancing Wu Li Masters | The Dancing Wu Li Masters The Dancing Wu Li Masters is a 1979 book by Gary Zukav, a popular science work exploring modern physics, and quantum phenomena in particular. It was awarded a 1980 U.S. National Book Award in category of Science. Although it explores empirical topics in modern physics research, \"The Dancing Wu Li Masters\" gained attention for leveraging metaphors taken from eastern spiritual movements, in particular the Huayen school of Buddhism with the monk Fazang\\'s treatise on The Golden Lion, to explain quantum phenomena and has been regarded by some reviewers as a New Age work, although the', 'Markus Zusak | a runner-up for the Printz Award in America. \"The Book Thief\" was published in 2005 and has since been translated into more than 30 languages. \"The Book Thief\" was adapted as a film of the same name in 2013. \"The Messenger\" (\"I Am the Messenger\" in the United States) was published in 2002 and was one of Zusak\\'s first novels. This novel has won awards such as the New South Wales Premier\\'s Literary Awards: Ethel Turner Prize for Young People\\'s Literature. In March 2016 Zusak talked about his unfinished novel \"Bridge of Clay.\" He stated that the book was 90%']\n)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e1f577-408c-4ede-9e27-65a24aafca5f",
   "metadata": {},
   "source": [
    "If we need passages with scores and other metadata, we can call `rm` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "37f4ff3d-de41-4fc7-8943-6dfd894dd1f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:09.163986Z",
     "start_time": "2024-02-25T18:09:09.134421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[{'pid': 7182463,\n  'prob': 1.0,\n  'rank': 1,\n  'score': 24.77630615234375,\n  'text': 'Gary Zukav | Gary Zukav Gary Zukav (born October 17, 1942) is an American spiritual teacher and the author of four consecutive New York Times Best Sellers. Beginning in 1998, he appeared more than 30 times on \"The Oprah Winfrey Show\" to discuss transformation in human consciousness concepts presented in his book \"The Seat of the Soul\". His first book, \"The Dancing Wu Li Masters\" (1979), won a U.S. National Book Award. Gary Zukav was born in Port Arthur, Texas, and spent his early childhood in San Antonio and Houston. His family moved to Pittsburg, Kansas, while he was in fourth grade. In',\n  'long_text': 'Gary Zukav | Gary Zukav Gary Zukav (born October 17, 1942) is an American spiritual teacher and the author of four consecutive New York Times Best Sellers. Beginning in 1998, he appeared more than 30 times on \"The Oprah Winfrey Show\" to discuss transformation in human consciousness concepts presented in his book \"The Seat of the Soul\". His first book, \"The Dancing Wu Li Masters\" (1979), won a U.S. National Book Award. Gary Zukav was born in Port Arthur, Texas, and spent his early childhood in San Antonio and Houston. His family moved to Pittsburg, Kansas, while he was in fourth grade. In'}]"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm(\"Which award did Gary Zukav's first book receive?\", k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2017ee-1375-4251-a24f-7f792852ffac",
   "metadata": {},
   "source": [
    "## Few-shot OpenQA with context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c0997-e624-4fc0-824d-e13066978b0a",
   "metadata": {},
   "source": [
    "Let's build on the above core concepts to define a basic retrieval-augmented generation (RAG) program. This program solves the core task of few-shot OpenQA task and will serve as the basis for the homework questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714dd71-02aa-4e84-840f-807b4e501732",
   "metadata": {},
   "source": [
    "We begin with a signature that takes context into account but is otherwise just like `BasicQASignature` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bcd35bc4-9bbc-4286-ad6b-d7474b51423e",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:09.867942Z",
     "start_time": "2024-02-25T18:09:09.860036Z"
    }
   },
   "outputs": [],
   "source": [
    "class ContextQASignature(dspy.Signature):\n",
    "    __doc__ = \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de481d-d4dd-42c3-99c1-7a112c7f521f",
   "metadata": {},
   "source": [
    "And here is a complete program/system for the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f84481c9-6e6a-4331-a5e1-cf2b9e27b082",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:10.030193Z",
     "start_time": "2024-02-25T18:09:10.021104Z"
    }
   },
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=1):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.Predict(ContextQASignature)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7c4c1ddd-455e-4ad5-b1b1-bf1267364660",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:10.112375Z",
     "start_time": "2024-02-25T18:09:10.105453Z"
    }
   },
   "outputs": [],
   "source": [
    "rag_model = RAG(num_passages=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7fce28fa-aa8a-4cec-a07a-5365a5eb32df",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:10.206451Z",
     "start_time": "2024-02-25T18:09:10.198129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Prediction(\n    context=['Gary Zukav | Gary Zukav Gary Zukav (born October 17, 1942) is an American spiritual teacher and the author of four consecutive New York Times Best Sellers. Beginning in 1998, he appeared more than 30 times on \"The Oprah Winfrey Show\" to discuss transformation in human consciousness concepts presented in his book \"The Seat of the Soul\". His first book, \"The Dancing Wu Li Masters\" (1979), won a U.S. National Book Award. Gary Zukav was born in Port Arthur, Texas, and spent his early childhood in San Antonio and Houston. His family moved to Pittsburg, Kansas, while he was in fourth grade. In', 'The Dancing Wu Li Masters | The Dancing Wu Li Masters The Dancing Wu Li Masters is a 1979 book by Gary Zukav, a popular science work exploring modern physics, and quantum phenomena in particular. It was awarded a 1980 U.S. National Book Award in category of Science. Although it explores empirical topics in modern physics research, \"The Dancing Wu Li Masters\" gained attention for leveraging metaphors taken from eastern spiritual movements, in particular the Huayen school of Buddhism with the monk Fazang\\'s treatise on The Golden Lion, to explain quantum phenomena and has been regarded by some reviewers as a New Age work, although the', 'Markus Zusak | a runner-up for the Printz Award in America. \"The Book Thief\" was published in 2005 and has since been translated into more than 30 languages. \"The Book Thief\" was adapted as a film of the same name in 2013. \"The Messenger\" (\"I Am the Messenger\" in the United States) was published in 2002 and was one of Zusak\\'s first novels. This novel has won awards such as the New South Wales Premier\\'s Literary Awards: Ethel Turner Prize for Young People\\'s Literature. In March 2016 Zusak talked about his unfinished novel \"Bridge of Clay.\" He stated that the book was 90%'],\n    answer='U.S. National Book Award'\n)"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_model(question=\"Which award did Gary Zukav's first book receive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873286e4-aaa7-4359-a5f2-04f8cbcceac8",
   "metadata": {},
   "source": [
    "An optional tiny evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "25715550-2ba8-44bb-9a11-ca3bc3e0af56",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:11.600638Z",
     "start_time": "2024-02-25T18:09:11.564802Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 15  (13.3): 100%|██████████| 15/15 [00:00<00:00, 2107.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 15  (13.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x2b7e08d60>",
      "text/html": "<style type=\"text/css\">\n#T_e0be0 th {\n  text-align: left;\n}\n#T_e0be0 td {\n  text-align: left;\n}\n#T_e0be0_row0_col0, #T_e0be0_row0_col1, #T_e0be0_row0_col2, #T_e0be0_row0_col3, #T_e0be0_row0_col4, #T_e0be0_row1_col0, #T_e0be0_row1_col1, #T_e0be0_row1_col2, #T_e0be0_row1_col3, #T_e0be0_row1_col4, #T_e0be0_row2_col0, #T_e0be0_row2_col1, #T_e0be0_row2_col2, #T_e0be0_row2_col3, #T_e0be0_row2_col4, #T_e0be0_row3_col0, #T_e0be0_row3_col1, #T_e0be0_row3_col2, #T_e0be0_row3_col3, #T_e0be0_row3_col4, #T_e0be0_row4_col0, #T_e0be0_row4_col1, #T_e0be0_row4_col2, #T_e0be0_row4_col3, #T_e0be0_row4_col4 {\n  text-align: left;\n  white-space: pre-wrap;\n  word-wrap: break-word;\n  max-width: 400px;\n}\n</style>\n<table id=\"T_e0be0\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_e0be0_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n      <th id=\"T_e0be0_level0_col1\" class=\"col_heading level0 col1\" >example_answer</th>\n      <th id=\"T_e0be0_level0_col2\" class=\"col_heading level0 col2\" >context</th>\n      <th id=\"T_e0be0_level0_col3\" class=\"col_heading level0 col3\" >pred_answer</th>\n      <th id=\"T_e0be0_level0_col4\" class=\"col_heading level0 col4\" >answer_exact_match</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_e0be0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_e0be0_row0_col0\" class=\"data row0 col0\" >In 1517 who was Luther's bishop?</td>\n      <td id=\"T_e0be0_row0_col1\" class=\"data row0 col1\" >Albert of Mainz</td>\n      <td id=\"T_e0be0_row0_col2\" class=\"data row0 col2\" >[\"Paul Speratus | Ellwangen, Priest of the Diocese of Augsburg). Early studies took him to Paris and Italy, as well as (probably) Freiburg and Vienna....</td>\n      <td id=\"T_e0be0_row0_col3\" class=\"data row0 col3\" >Johann Eck</td>\n      <td id=\"T_e0be0_row0_col4\" class=\"data row0 col4\" >False</td>\n    </tr>\n    <tr>\n      <th id=\"T_e0be0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_e0be0_row1_col0\" class=\"data row1 col0\" >When was the construction that changed the Rhine's Delta?</td>\n      <td id=\"T_e0be0_row1_col1\" class=\"data row1 col1\" >20th Century</td>\n      <td id=\"T_e0be0_row1_col2\" class=\"data row1 col2\" >['Rhine | rivers and streams. Many rivers have been closed (\"dammed\") and now serve as drainage channels for the numerous polders. The construction of Delta...</td>\n      <td id=\"T_e0be0_row1_col3\" class=\"data row1 col3\" >second half of the 20th Century</td>\n      <td id=\"T_e0be0_row1_col4\" class=\"data row1 col4\" >False</td>\n    </tr>\n    <tr>\n      <th id=\"T_e0be0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_e0be0_row2_col0\" class=\"data row2 col0\" >How many companies were registered in Warsaw in 2006?</td>\n      <td id=\"T_e0be0_row2_col1\" class=\"data row2 col1\" >304,016</td>\n      <td id=\"T_e0be0_row2_col2\" class=\"data row2 col2\" >['Warsaw | such as Sydney, Istanbul, Amsterdam or Seoul. Warsaw, especially its city centre (\"Śródmieście\"), is home not only to many national institutions and government...</td>\n      <td id=\"T_e0be0_row2_col3\" class=\"data row2 col3\" >304,016</td>\n      <td id=\"T_e0be0_row2_col4\" class=\"data row2 col4\" >✔️ [True]</td>\n    </tr>\n    <tr>\n      <th id=\"T_e0be0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_e0be0_row3_col0\" class=\"data row3 col0\" >What is the CJEU's duty?</td>\n      <td id=\"T_e0be0_row3_col1\" class=\"data row3 col1\" >to \"ensure that in the interpretation and application of the Treaties the law is observed\"</td>\n      <td id=\"T_e0be0_row3_col2\" class=\"data row3 col2\" >['European Union law | elected by the judges for three years. While TEU article 19(3) says the Court of Justice is the ultimate court to...</td>\n      <td id=\"T_e0be0_row3_col3\" class=\"data row3 col3\" >To ensure that \"the law is observed\"</td>\n      <td id=\"T_e0be0_row3_col4\" class=\"data row3 col4\" >False</td>\n    </tr>\n    <tr>\n      <th id=\"T_e0be0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_e0be0_row4_col0\" class=\"data row4 col0\" >What would a teacher do for someone who is cocky?</td>\n      <td id=\"T_e0be0_row4_col1\" class=\"data row4 col1\" >deflate</td>\n      <td id=\"T_e0be0_row4_col2\" class=\"data row4 col2\" >['Teacher | for the individual students accordingly. For example, an experienced teacher and parent described the place of a teacher in learning as follows: \"The...</td>\n      <td id=\"T_e0be0_row4_col3\" class=\"data row4 col3\" >deflate the cocky</td>\n      <td id=\"T_e0be0_row4_col4\" class=\"data row4 col4\" >False</td>\n    </tr>\n  </tbody>\n</table>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                <div style='\n                    text-align: center; \n                    font-size: 16px; \n                    font-weight: bold; \n                    color: #555; \n                    margin: 10px 0;'>\n                    ... 10 more rows not displayed ...\n                </div>\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "13.33"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_evaluater(rag_model, metric=answer_exact_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb28556-d901-4b6b-8a7d-93040203294d",
   "metadata": {},
   "source": [
    "## Question 1: Optimizing RAG [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f481081-3e16-4ccd-b379-c7e0c3011286",
   "metadata": {},
   "source": [
    "We used `RAG` above as a zero-shot system. We could turn it into a few-shot system by using `LabeledFewShot` as we did in [the teleprompting section](#Teleprompting) above, but this may actually be problematic: if we randomly sample demonstrations with retrieved passages, we might be instructing the model with a lot of cases where the context passage isn't helping (and may actually be actively misleading the model). \n",
    "\n",
    "What we'd like to do is select demonstrations where the model gets the answer correct and the context passage does contain the answer. To do this, we will use the DSPy `BootstrapFewShot` optimizer. There are two steps for this: (1) defining a metric and (2) running the optimizer.\n",
    "\n",
    "__Note__: The code for this question can be found in the DSPy tutorials, and you should feel free to make use of that code. The goal is to help you understand the design patterns and overall logic of optimizing DSPy programs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ae027-f51a-4607-822f-2a721acde73c",
   "metadata": {},
   "source": [
    "__Task 1__: Complete `validate_context_and_answer` according to the specification in the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ad09f428-1a68-4cef-9e7a-7c6faf9244a4",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:16.316923Z",
     "start_time": "2024-02-25T18:09:16.311202Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate_context_and_answer(example:dspy.Example, pred:dspy.Prediction, trace=None):\n",
    "    \"\"\"Return True if `example.answer` matches `pred.answer` according\n",
    "    to `dspy.evaluate.answer_exact_match` and `pred.context` contains\n",
    "    `example.answer` according to `dspy.evaluate.answer_exact_match`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    example: dspy.Example \n",
    "        with attributes `answer` and `context`\n",
    "    pred: dspy.Example \n",
    "        with attributes `answer` and `context`\n",
    "    trace : None (included for dspy internal compatibility)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    ##### YOUR CODE HERE\n",
    "    answers_match = answer_exact_match(example=example, pred=pred)\n",
    "    context_match = dspy.evaluate.answer_passage_match(example=example, pred=pred)\n",
    "    \n",
    "    return answers_match and context_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0918c5f-2864-4856-adcf-ccb94aca6108",
   "metadata": {},
   "source": [
    "A test you can use to check your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ddbca59c-7c11-4e4b-bec3-18a2ecde563b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:18.551581Z",
     "start_time": "2024-02-25T18:09:18.542415Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_validate_context_and_answer(func):\n",
    "    examples = [\n",
    "        (\n",
    "            dspy.Example(question=\"Q1\", answer=\"B\"),\n",
    "            dspy.Prediction(question=\"Q1\", context=\"A B C\", answer=\"B\"),\n",
    "            True\n",
    "        ),\n",
    "        # Context doesn't contain answer, but predicted answer is correct.\n",
    "        (\n",
    "            dspy.Example(question=\"Q1\", answer=\"D\"),\n",
    "            dspy.Prediction(question=\"Q1\", context=\"A B C\", answer=\"D\"),\n",
    "            False\n",
    "        ),\n",
    "        # Context contains answer, but predicted answer is not correct.\n",
    "        (\n",
    "            dspy.Example(question=\"Q1\", answer=\"C\"),\n",
    "            dspy.Prediction(question=\"Q1\", context=\"A B C\", answer=\"D\"),\n",
    "            False\n",
    "        )\n",
    "    ]\n",
    "    errcount = 0\n",
    "    for ex, pred, result in examples:\n",
    "        predicted = func(ex, pred, trace=None)\n",
    "        if predicted != result:\n",
    "            errcount += 1\n",
    "            print(f\"Error for `{func.__name__}`: \"\n",
    "                  f\"Expected inputs\\n\\t{ex}\\n\\t{pred} to return {result}.\")\n",
    "    if errcount == 0:\n",
    "        print(f\"No errors detected for `{func.__name__}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "38aa3141-ae8f-4bc6-a26e-64d108537612",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:19.131632Z",
     "start_time": "2024-02-25T18:09:19.122539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected for `validate_context_and_answer`\n"
     ]
    }
   ],
   "source": [
    "test_validate_context_and_answer(validate_context_and_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091ca79-ccbe-4429-bfe7-c4f1de118adf",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Task 2__: Complete `bootstrap_optimize` according to the specification in the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "129e7e92-033e-4f38-b309-94dac2b66d87",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:21.300471Z",
     "start_time": "2024-02-25T18:09:21.279097Z"
    }
   },
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "def bootstrap_optimize(model):\n",
    "    \"\"\"Use `BootstrapFewShot` to optimize `model`, with the metric set\n",
    "    to `validate_context_and_answer` as defined above and default\n",
    "    values for all other keyword arguments to `BootstrapFewShot`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: dspy.Module\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dspy.Module, the optimized version of `model`\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "    ##### YOUR CODE HERE\n",
    "    optimizer = dspy.teleprompt.BootstrapFewShot(metric=validate_context_and_answer)\n",
    "    return optimizer.compile(model, trainset=squad_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26af22-6d7d-4456-9e30-8f80f5b0b401",
   "metadata": {},
   "source": [
    "A test you can use to check your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "30dd9f56-31cc-4e45-8bc2-0fe56c3806a7",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:23.699458Z",
     "start_time": "2024-02-25T18:09:23.692044Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_bootstrap_optimize(func):\n",
    "    model = RAG()\n",
    "    compiled = func(model)\n",
    "    if not hasattr(compiled, \"_compiled\") or not compiled._compiled:\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "               \"The return value is not a compiled program.\")\n",
    "        return None\n",
    "    state = compiled.dump_state()\n",
    "    if not state['generate_answer']['demos']:\n",
    "        print(f\"Error for `{func.__name__}`: \"\n",
    "               \"The compiled program has no `demos`.\")\n",
    "        return None\n",
    "    print(f\"No errors detected for `{func.__name__}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "733c1c31-9c68-4d2a-a1c3-4eed0e1caa6f",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:09:24.243344Z",
     "start_time": "2024-02-25T18:09:24.192646Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/87599 [00:00<00:54, 1601.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 13 examples in round 0.\n",
      "No errors detected for `bootstrap_optimize`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_bootstrap_optimize(bootstrap_optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9971ffd0-a563-4d56-a896-0735a63ae92f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 2: Multi-passage summarization [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e4d85-05d7-4bc2-beaf-2434d4fe41da",
   "metadata": {},
   "source": [
    "The `dspy.Retrieve` layer in our `RAG` retrieves `k` passages, where `k` is under the control of the user. One hypothesis one might have is that it would be good to summarize these passages before using them as evidence. This seems especially likely to help in scenarios where the question can be answered only by synthesizing information across documents – it might be too much to ask the language model to do both synthesizing and answering in a single step.\n",
    "\n",
    "The current question maps out a basic strategy for summarization. The heart of it is a new signature called `SummarizeSignature`. This can be used on its own with a simple `dspy.Predict` call, and we'll incorporate it into a RAG program in the next question.\n",
    "\n",
    "For this question, though, your task is just to complete `SummarizeSignature`. The requirements are as follows:\n",
    "\n",
    "1. A `__doc__` value that gives an instruction that seems to work well. You can decide what to say here.\n",
    "2. A `dspy.InputField` named `context`. You can decide whether to use the `desc` parameter.\n",
    "3. A `dspy.OutputField` named `summary`. You can decide whether to use the `desc` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b7c080a6-d2e1-4d7a-ac60-5ef3a6931ad1",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:15:52.596011Z",
     "start_time": "2024-02-25T18:15:52.568863Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "summarize_len = \"several short, informationally dense sentences.\"\n",
    "\n",
    "class SummarizeSignature(dspy.Signature):\n",
    "    __doc__ = f\"Summarize the following documents from their original form into {summarize_len}\"\n",
    "    context = dspy.InputField(desc=\"The document to summarize.\")\n",
    "    summary = dspy.OutputField(desc=f\"The same document, summarized into {summarize_len}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6091f20-1cae-4e04-bd50-929271ae6a18",
   "metadata": {},
   "source": [
    "Here's a simple test that just checks for the required pieces in a basic way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "428c971a-2dcd-4344-afa0-8e52938ca4ee",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:15:53.317769Z",
     "start_time": "2024-02-25T18:15:53.308155Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_SummarizeSignature(sigclass):\n",
    "    fields = sigclass.fields\n",
    "    expected_fieldnames = ['context', 'summary']\n",
    "    fieldnames = sorted([field.input_variable for field in fields])\n",
    "    errcount = 0\n",
    "    if expected_fieldnames != fieldnames:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{sigclass.__name__}`: \"\n",
    "              f\"Expected fieldnames {expected_fieldnames}, got {fieldnames}.\")\n",
    "    if not sigclass.__doc__:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{sigclass.__name__}`: No docstring specified.\")\n",
    "    if errcount == 0:\n",
    "        print(f\"No errors detected for `{sigclass.__name__}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fec39e4d-b3ca-4355-9695-c9f61a24f3d2",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:15:53.482720Z",
     "start_time": "2024-02-25T18:15:53.471221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected for `SummarizeSignature`\n"
     ]
    }
   ],
   "source": [
    "test_SummarizeSignature(SummarizeSignature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ee5d0-2bd6-4cb0-873c-21f963a78555",
   "metadata": {},
   "source": [
    "Here is the simplest way to use `SummarizeSignature`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5e8129a3-90fb-4810-b18b-84e75525dd16",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:15:54.379384Z",
     "start_time": "2024-02-25T18:15:54.357506Z"
    }
   },
   "outputs": [],
   "source": [
    "summarizer = dspy.Predict(SummarizeSignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d8bfdb34-07d8-4488-a1ed-1b4788a2a119",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:15:56.607997Z",
     "start_time": "2024-02-25T18:15:54.758319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Prediction(\n    summary='The Guarani language, specifically Paraguayan Guarani, is an indigenous language spoken in South America and is one of the official languages of Paraguay. It is also spoken in neighboring countries like Argentina, Bolivia, and Brazil. There are different dialects of Guarani, including Eastern Bolivian Guarani and Western Bolivian Guarani, spoken in various regions. The Guarani language belongs to the Tupí-Guaraní branch and is spoken by over four million people across Brazil, Paraguay, Bolivia, and Argentina.'\n)"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(context=retriever(\"Where is Guarani spoken?\").passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7a44f-c50d-4674-9fbb-fcf216222e3a",
   "metadata": {},
   "source": [
    "## Question 3: Summarizing RAG [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812e189-2778-442f-81b1-577c303445a8",
   "metadata": {},
   "source": [
    "Your task for this question is to modify `RAG` as defined above so that the retrieved passages are summarized before being passed to `generate_answer`. \n",
    "\n",
    "Here is the `RAG` system copied from above with the class name changed to the one we will use for this new system. Your task is to add the summarization step. This should be very straightforward given the modular design that DSPy supports and encourages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fa956847-d6ff-40cf-bf02-9043863e548e",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:21:20.733911Z",
     "start_time": "2024-02-25T18:21:20.696455Z"
    }
   },
   "outputs": [],
   "source": [
    "class SummarizingRAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        # Please name your summarization later `summarize` so that we\n",
    "        # can check for its presence.\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        ##### YOUR CODE HERE\n",
    "        self.summarize = dspy.Predict(SummarizeSignature)\n",
    "\n",
    "\n",
    "        self.generate_answer = dspy.Predict(ContextQASignature)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        ##### YOUR CODE HERE\n",
    "        context = self.summarize(context=context).summary\n",
    "\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b65d8-14ca-4dbe-a815-a0d00940b0b4",
   "metadata": {},
   "source": [
    "A simple test for this design spec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "824f063d-1735-4bf3-9337-e9728a5b7800",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:20:54.647740Z",
     "start_time": "2024-02-25T18:20:54.619133Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_SummarizingRAG(classname):\n",
    "    model = classname(num_passages=3)\n",
    "    errcount = 0\n",
    "    if not hasattr(model, \"summarize\"):\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{classname.__name__}`: \"\n",
    "              f\"Expected a layer called 'summarize'\")\n",
    "    context = model.retrieve(\"What are some foods?\").passages\n",
    "    pred = model(\"What are some foods?\")\n",
    "    if context == pred.context:\n",
    "        errcount += 1\n",
    "        print(f\"Error for `{classname.__name__}`: \"\n",
    "              \"The model seems to be using raw retrieved contexts \"\n",
    "              \"for predictions rather than summarizing them.\")\n",
    "    if errcount == 0:\n",
    "        print(f\"No errors detected for `{classname.__name__}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8e1d38c4-540c-4473-b464-0061b5b8a09c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:20:56.075720Z",
     "start_time": "2024-02-25T18:20:55.401865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors detected for `SummarizingRAG`\n"
     ]
    }
   ],
   "source": [
    "test_SummarizingRAG(SummarizingRAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3b6ad-b960-4b8f-8137-a9b90953b2fc",
   "metadata": {},
   "source": [
    "Model usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "95e03778-5a7f-4119-ac62-f4965bfe9a8c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:21:25.712736Z",
     "start_time": "2024-02-25T18:21:25.688320Z"
    }
   },
   "outputs": [],
   "source": [
    "summarizing_rag_model = SummarizingRAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5077370b-156f-4738-948b-86d832b2ebbd",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-25T18:21:29.471935Z",
     "start_time": "2024-02-25T18:21:26.015573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gary Zukav | Gary Zukav Gary Zukav (born October 17, 1942) is an American spiritual teacher and the author of four consecutive New York Times Best Sellers. Beginning in 1998, he appeared more than 30 times on \"The Oprah Winfrey Show\" to discuss transformation in human consciousness concepts presented in his book \"The Seat of the Soul\". His first book, \"The Dancing Wu Li Masters\" (1979), won a U.S. National Book Award. Gary Zukav was born in Port Arthur, Texas, and spent his early childhood in San Antonio and Houston. His family moved to Pittsburg, Kansas, while he was in fourth grade. In', 'The Dancing Wu Li Masters | The Dancing Wu Li Masters The Dancing Wu Li Masters is a 1979 book by Gary Zukav, a popular science work exploring modern physics, and quantum phenomena in particular. It was awarded a 1980 U.S. National Book Award in category of Science. Although it explores empirical topics in modern physics research, \"The Dancing Wu Li Masters\" gained attention for leveraging metaphors taken from eastern spiritual movements, in particular the Huayen school of Buddhism with the monk Fazang\\'s treatise on The Golden Lion, to explain quantum phenomena and has been regarded by some reviewers as a New Age work, although the', 'Markus Zusak | a runner-up for the Printz Award in America. \"The Book Thief\" was published in 2005 and has since been translated into more than 30 languages. \"The Book Thief\" was adapted as a film of the same name in 2013. \"The Messenger\" (\"I Am the Messenger\" in the United States) was published in 2002 and was one of Zusak\\'s first novels. This novel has won awards such as the New South Wales Premier\\'s Literary Awards: Ethel Turner Prize for Young People\\'s Literature. In March 2016 Zusak talked about his unfinished novel \"Bridge of Clay.\" He stated that the book was 90%']\n",
      "Gary Zukav is an American spiritual teacher and author of four New York Times Best Sellers, known for discussing transformation in human consciousness on \"The Oprah Winfrey Show\". His first book, \"The Dancing Wu Li Masters\", won a U.S. National Book Award in 1979. The book explores modern physics and quantum phenomena using metaphors from eastern spiritual movements. Markus Zusak is an Australian author known for his books \"The Book Thief\" and \"The Messenger\", with the former being adapted into a film in 2013. Zusak has also discussed his unfinished novel \"Bridge of Clay\" in 2016.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Prediction(\n    context='Gary Zukav is an American spiritual teacher and author of four New York Times Best Sellers, known for discussing transformation in human consciousness on \"The Oprah Winfrey Show\". His first book, \"The Dancing Wu Li Masters\", won a U.S. National Book Award in 1979. The book explores modern physics and quantum phenomena using metaphors from eastern spiritual movements. Markus Zusak is an Australian author known for his books \"The Book Thief\" and \"The Messenger\", with the former being adapted into a film in 2013. Zusak has also discussed his unfinished novel \"Bridge of Clay\" in 2016.',\n    answer='U.S. National Book Award'\n)"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizing_rag_model(question=\"Which award did Gary Zukav's first book receive?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ebc3e9-7baf-4e1c-81a0-8c1e3588a882",
   "metadata": {},
   "source": [
    "Note: if you decide to use `BootstrapFewShot` on this, be sure not to use the metric we defined above, which requires that the passage embeds the correct answer as a substring. Now that we are summarizing, this is unlikely to hold, even if the answers are good ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19655d70-007c-4a41-9f3b-e20df9c5169b",
   "metadata": {},
   "source": [
    "## Question 4: Your original system [3 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d268f-70e1-4325-a2ff-7361d73788b9",
   "metadata": {},
   "source": [
    "This question asks you to design your own few-shot OpenQA system. All of the code above can be used and modified for this, and the requirement is just that you try something new that goes beyond what we've done so far. \n",
    "\n",
    "Terms for the bake-off:\n",
    "\n",
    "* You can make free use of SQuAD and other publicly available data.\n",
    "\n",
    "* The LM must be an autoregressive language model. No trained QA components can be used. This includes general purpose LMs that have been fine-tuned for QA. (We have obviously waded into some vague territory here. The spirit of this is to make use of frozen, general-purpose models. We welcome questions about exactly how this is defined, since it could be instructive to explore this.)\n",
    "\n",
    "Here are some ideas for the original system:\n",
    "\n",
    "* We have relied almost entirely on `dspy.Predict`. Drop-in replacements include `dspy.ChainOfThought` and `dspy.ReAct`.\n",
    "\n",
    "* We have used only one retriever. DSPy supports other retrieval mechanisms, including retrieval using [You.com](https://you.com/).\n",
    "\n",
    "* DSPy includes additional optimizers. Two that are worth trying are `SignatureOptimizer` for automatic prompt exploration and `BootstrapFewShotWithRandomSearch`, which combines `LabeledFewShot` and `BootstrapFewShot`,\n",
    "\n",
    "* Our one-step summarization procedure from Question 3 doesn't change the query to the retriever. We might want it to change as we gather evidence. This is a common design principle for multi-hop OpenQA systems.\n",
    "\n",
    "__Original system instructions__:\n",
    "\n",
    "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b557f3c3-ee72-480e-9d99-9095372f99c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T23:09:23.071964Z",
     "start_time": "2024-02-25T23:09:16.083140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/mgbvox/dspy@yourm-add-k\r\n",
      "  Cloning https://github.com/mgbvox/dspy (to revision yourm-add-k) to /private/var/folders/44/892q_9xd3tn6_kjf8zk82brc0000gn/T/pip-req-build-cblpws5p\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/mgbvox/dspy /private/var/folders/44/892q_9xd3tn6_kjf8zk82brc0000gn/T/pip-req-build-cblpws5p\r\n",
      "  Running command git checkout -b yourm-add-k --track origin/yourm-add-k\r\n",
      "  Switched to a new branch 'yourm-add-k'\r\n",
      "  branch 'yourm-add-k' set up to track 'origin/yourm-add-k'.\r\n",
      "  Resolved https://github.com/mgbvox/dspy to commit 027fb92d0c4bcade5ca50e19fcbdc9c441544b01\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hBuilding wheels for collected packages: dspy-ai\r\n",
      "  Building wheel for dspy-ai (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for dspy-ai: filename=dspy_ai-2.3.1-py3-none-any.whl size=165117 sha256=eeebde53636a189ec09941ad37aa43c7593b3caee0db980f53810fc4db10317e\r\n",
      "  Stored in directory: /private/var/folders/44/892q_9xd3tn6_kjf8zk82brc0000gn/T/pip-ephem-wheel-cache-cql31adp/wheels/5e/e6/44/d143cbe4d3201b15259cfbac402ed9687b687216fe683461c6\r\n",
      "Successfully built dspy-ai\r\n",
      "Installing collected packages: dspy-ai\r\n",
      "  Attempting uninstall: dspy-ai\r\n",
      "    Found existing installation: dspy-ai 2.3.1\r\n",
      "    Uninstalling dspy-ai-2.3.1:\r\n",
      "      Successfully uninstalled dspy-ai-2.3.1\r\n",
      "Successfully installed dspy-ai-2.3.1\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# PLEASE MAKE SURE TO INCLUDE THE FOLLOWING BETWEEN THE START AND STOP COMMENTS:\n",
    "#   1) Textual description of your system.\n",
    "#   2) The code for your original system.\n",
    "# PLEASE MAKE SURE NOT TO DELETE OR EDIT THE START AND STOP COMMENTS\n",
    "\n",
    "# START COMMENT: Enter your system description in this cell.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "IMPORTANT - make sure you're on the nightly release of dspy\n",
    "Part of my work for this assignment involved addressing several bugs in the YouRM Retriever\n",
    "model; see PRs: \n",
    "https://github.com/stanfordnlp/dspy/pull/453\n",
    "https://github.com/stanfordnlp/dspy/pull/455\n",
    "https://github.com/stanfordnlp/dspy/pull/467\n",
    "\n",
    "One pending PR (https://github.com/stanfordnlp/dspy/pull/467) has yet to be merged at time of submission,\n",
    "so I recommend you install from my fork if you intend to actually run this, e.g.: \n",
    "! pip install --force-reinstall --no-deps git+https://github.com/mgbvox/dspy@main\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "This implements a Multi-Hop (\"Baleen\"-esque) question-answering RAG model.\n",
    "\n",
    "YouRM was initially used as the retriever model with GPT-4-turbo as our LLM;\n",
    "however, I ran out of YouRM API credit before I was able to fine tune the model \n",
    "and can't afford the $100/month for premium access. \n",
    "\n",
    "I reverted to GPT-3.5-Turbo for cost related reasons as well.\n",
    "\n",
    "While I intended to fallback to ColbertV2 as an rm, the ColbertV2 server was down for the \n",
    "majority of development as well, so I used a secondary Colbert model trained on \n",
    "wikipedia only for the fallback-fallback rm.\n",
    "\n",
    "I expect question answering suffered significantly because of this. At least I was able to \n",
    "get in a bunch of updates to the YouRM source code before having to fall back!\n",
    "\"\"\"\n",
    "\n",
    "import functools\n",
    "import hashlib\n",
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional\n",
    "\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, SecretStr\n",
    "\n",
    "import dspy\n",
    "from dsp.utils import deduplicate\n",
    "from dspy.evaluate import answer_exact_match\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# a convenience wrapper for settings (I don't like untyped dicts when they can be avoided!)\n",
    "class Settings(BaseModel):\n",
    "    OPENAI_API_KEY: SecretStr\n",
    "    YDC_API_KEY: SecretStr\n",
    "    LLM_MODEL: str\n",
    "    COLBERT_SERVER: str\n",
    "\n",
    "    @classmethod\n",
    "    def from_dotenv(cls):\n",
    "        load_dotenv()\n",
    "        return cls.parse_obj(os.environ)\n",
    "\n",
    "\n",
    "_cache_root = Path.cwd() / \".disk_cache\"\n",
    "os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(\".\", \"cache_2\")\n",
    "\n",
    "\n",
    "def deterministic_hash(string: str) -> str:\n",
    "    return hashlib.sha256(string.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def safe_to_string(obj: object) -> str:\n",
    "    as_str = str(obj)\n",
    "    if \"<\" in as_str:\n",
    "        # The string repr of the object is ClassName<memory address>\n",
    "        # This is non-deterministic (at least, the memory part is)\n",
    "        # So just use its class name\n",
    "        return obj.__class__.__name__\n",
    "    return as_str\n",
    "\n",
    "# AI api calls can be expensive; this lets us cache our work on disk\n",
    "def disk_cache(fn: Callable) -> Callable:\n",
    "    \"\"\"A very basic disk caching decorator; cache function call\n",
    "    results on disk indexed by the argument values to said function.\"\"\"\n",
    "    if not _cache_root.exists():\n",
    "        _cache_root.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    cache_dir = _cache_root / (fn.__name__ if fn.__name__ else \"lambda\")\n",
    "    if not cache_dir.exists():\n",
    "        cache_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        key = \",\".join(\n",
    "            [safe_to_string(arg) for arg in args]\n",
    "            + [f\"{k}:{safe_to_string(v)}\" for k, v in kwargs.items()]\n",
    "        )\n",
    "        key = deterministic_hash(key)\n",
    "\n",
    "        cache_file = cache_dir / key\n",
    "\n",
    "        if cache_file.exists():\n",
    "            with cache_file.open(\"rb\") as f:\n",
    "                cached_result = pickle.load(f)\n",
    "                return cached_result\n",
    "        else:\n",
    "            result = fn(*args, **kwargs)\n",
    "            with cache_file.open(\"wb\") as f:\n",
    "                pickle.dump(result, f)\n",
    "            return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@disk_cache\n",
    "def get_results_from_rm(question: str, rm: dspy.Retrieve) -> dspy.Prediction:\n",
    "    \"\"\"YouRM is expensive, but calls to this service are not natively cached.\n",
    "    To save money, I'm caching web search results here, under the assumption\n",
    "    that relevant information for each question doesn't change frequently enough\n",
    "    to merit multiple calls for the same question.\n",
    "    \"\"\"\n",
    "    results = rm(question)\n",
    "    return results\n",
    "\n",
    "\n",
    "class MultiHopQuerySignature(dspy.Signature):\n",
    "    \"\"\"Given a question and some context, generate a web query to increase the relevance of the context for answering the question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(\n",
    "        desc=\"Already-gathered context to be used in answering the question.\"\n",
    "    )\n",
    "    hop = dspy.InputField(\n",
    "        desc=\"Stage in the search process - hop/total_hops. When hop == total_hops, no more queries may be performed and the question must be answered.\"\n",
    "    )\n",
    "    question = dspy.InputField(desc=\"The question we're trying to answer.\")\n",
    "    previous_queries = dspy.InputField(\n",
    "        desc=\"Previous queries; the query you generate should be distinct.\"\n",
    "    )\n",
    "    query = dspy.OutputField(\n",
    "        desc=\"A concise web query designed to gather precise, relevant context for use in answering the above question.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class WebAnswerSignature(dspy.Signature):\n",
    "    \"\"\"Use the following information, gathered from the web, to answer the question succinctly.\n",
    "\n",
    "    Your response should be concise without sacrificing precision.\n",
    "    \"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"Context from a web search.\")\n",
    "    question = dspy.InputField(desc=\"The question to answer.\")\n",
    "    answer = dspy.OutputField(\n",
    "        desc=\"The answer to the question, given our web search context.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# REMOVED - dspy.Assert/Suggest not playing nice\n",
    "# def validate_query_distinction_local(previous_queries: list[str], query: str):\n",
    "#     \"\"\"check if query is distinct from previous queries\"\"\"\n",
    "#     if not previous_queries:\n",
    "#         return True\n",
    "#     if dspy.evaluate.answer_exact_match_str(query, previous_queries, frac=0.8):\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "\n",
    "class WebRagMultiHop(dspy.Module):\n",
    "    def __init__(self, num_hops: int = 3, passages_per_hop: int = 5) -> None:\n",
    "        super().__init__()\n",
    "        # ensure passages_per_hop is a list of len num_hops\n",
    "\n",
    "        self.num_hops = num_hops\n",
    "        self.passages_per_hop = passages_per_hop\n",
    "\n",
    "        # Multiple query generation steps\n",
    "        self.generate_queries = [\n",
    "            dspy.ChainOfThought(MultiHopQuerySignature) for _ in range(num_hops)\n",
    "        ]\n",
    "        # allow different numbers of documents to be retrieved at different stages in the hop process\n",
    "        self.retriever = dspy.Retrieve(k=passages_per_hop)\n",
    "        # final answer gen step\n",
    "        self.generate_answer = dspy.ChainOfThought(WebAnswerSignature)\n",
    "\n",
    "    def forward(self, question: str) -> dspy.Prediction:\n",
    "        context: list[str] = []\n",
    "        previous_queries = [question]\n",
    "        for hop in range(self.num_hops):\n",
    "            query_gen = self.generate_queries[hop]\n",
    "            hop_progress = f\"{hop+1}/{self.num_hops}\"\n",
    "            query = query_gen(\n",
    "                context=context,\n",
    "                hop=hop_progress,\n",
    "                question=question,\n",
    "                previous_queries=\"; \".join(previous_queries),\n",
    "            ).query\n",
    "\n",
    "            # eliminate quote wrappers - mess with YouRM query api\n",
    "            query = query.replace(\"'\", \"\").replace('\"', \"\")\n",
    "\n",
    "            # dspy assertions appear to be bugged\n",
    "            \"\"\"dspy.Suggest(\n",
    "                len(query) <= 100,\n",
    "                \"Query should be short and less than 100 characters.\",\n",
    "            )\n",
    "            dspy.Suggest(\n",
    "                len(query) >= 5,\n",
    "                \"Query should be longer than 5 characters.\",\n",
    "            )\n",
    "            dspy.Suggest(\n",
    "                validate_query_distinction_local(previous_queries, query),\n",
    "                \"Query should be distinct from: \"\n",
    "                + \"; \".join(f\"{i+1}) {q}\" for i, q in enumerate(previous_queries)),\n",
    "            )\"\"\"\n",
    "\n",
    "            previous_queries.append(query)\n",
    "            passages = get_results_from_rm(query, rm=self.retriever).passages\n",
    "\n",
    "            # dspy.Assert(\n",
    "            #     len(passages) > 0, \"Web search must return at least one passage.\"\n",
    "            # )\n",
    "            context = deduplicate(context + passages)\n",
    "        pred = self.generate_answer(context=context, question=question)\n",
    "        answer = pred.answer\n",
    "\n",
    "        # dspy.Suggest(\n",
    "        #     len(answer) <= 300, \"Answer should be short and less than 300 characters\"\n",
    "        # )\n",
    "        print(f\"Len CTX: {len(context)} --> Question: {question} --> Answer: {answer}\")\n",
    "        return dspy.Prediction(context=context, answer=answer)\n",
    "\n",
    "\n",
    "def get_squad_split(squad, split=\"validation\", limit: int = -1):\n",
    "    \"\"\"\n",
    "    Use `split='train'` for the train split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dspy.Example with attributes question, answer\n",
    "\n",
    "    \"\"\"\n",
    "    data = zip(*[squad[split][field] for field in squad[split].features])\n",
    "    exs = [\n",
    "        dspy.Example(question=q, answer=a[\"text\"][0]).with_inputs(\"question\")\n",
    "        for eid, title, context, q, a in data\n",
    "    ]\n",
    "    return exs[:limit]\n",
    "\n",
    "\n",
    "def setup():\n",
    "    settings = Settings.from_dotenv()\n",
    "    # Ran out of free YouDC API queries; definitely NOT paying $100/month for this.\n",
    "    # So, looks like we're using Colbert :(\n",
    "    # rm = YouRM(ydc_api_key=settings.YDC_API_KEY.get_secret_value())\n",
    "    # Colbert was FURTHER broken (service was down), so we wound up using the wikipedia version (see slack thread)\n",
    "    rm = dspy.ColBERTv2(\n",
    "        url=settings.COLBERT_SERVER\n",
    "    )\n",
    "    # Used 3.5-turbo to save on costs (really, Stanford should foot the bill for learners, we're already paying a lot for this class)\n",
    "    lm = dspy.OpenAI(\n",
    "        model=settings.LLM_MODEL,\n",
    "        api_key=settings.OPENAI_API_KEY.get_secret_value(),\n",
    "    )\n",
    "    dspy.settings.configure(lm=lm, rm=rm)\n",
    "\n",
    "\n",
    "def train():\n",
    "    setup()\n",
    "\n",
    "    # Would normally construct our model and activate assertions/backtracking\n",
    "    # see https://dspy-docs.vercel.app/docs/building-blocks/assertions\n",
    "    # However, assertions are buggy in the main branch, so no assertions for training as of now.\n",
    "    # from dspy.primitives.assertions import assert_transform_module, backtrack_handler\n",
    "    # web_rag = assert_transform_module(WebRagMultiHop(), backtrack_handler)\n",
    "    web_rag = WebRagMultiHop()\n",
    "\n",
    "    # This is mostly taken verbatim from the vercel docs;\n",
    "    optimizer_config = dict(\n",
    "        max_bootstrapped_demos=3,\n",
    "        max_labeled_demos=3,\n",
    "        num_candidate_programs=10,\n",
    "        num_threads=32,\n",
    "    )\n",
    "    optimizer = BootstrapFewShotWithRandomSearch(\n",
    "        metric=functools.partial(answer_exact_match, frac=0.5), **optimizer_config\n",
    "    )\n",
    "\n",
    "    squad = load_dataset(\"squad\")\n",
    "    # Used a much smaller dataset (to save on time and cost)\n",
    "    trainset = random.sample(get_squad_split(squad, split=\"train\"), k=1000)\n",
    "\n",
    "    web_rag_compiled = optimizer.compile(web_rag, trainset=trainset)\n",
    "\n",
    "    # dump out the model for later loading\n",
    "    datetime_file_safe = datetime.now().strftime(\"%Y_%m_%d-%H_%M\")\n",
    "    web_rag_compiled.save(f\"web_rag_compiled-{datetime_file_safe}.json\")\n",
    "\n",
    "# cache queries to the model to disk so we can pick up where we left off (in the event of a network failure, runtime errors, etc)\n",
    "@disk_cache\n",
    "def answer_bakeoff_question(question: str, model: dspy.Module) -> str:\n",
    "    result = model(question)\n",
    "    return result.answer\n",
    "\n",
    "# a simple wrapper around queries to the model\n",
    "# this allows multiprocessing.Pool mapping to speed up answering\n",
    "def do_bake(question: str) -> str:\n",
    "    setup()\n",
    "    web_rag = WebRagMultiHop()\n",
    "    answer = answer_bakeoff_question(question, web_rag)\n",
    "\n",
    "    print(question, \"--->\", answer)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# actually answer the questions!\n",
    "def bake():\n",
    "    questions: list[str] = (\n",
    "        Path(\"data/openqa/cs224u-openqa-test-unlabeled.txt\").read_text().splitlines()\n",
    "    )\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        pool.map(do_bake, questions)\n",
    "\n",
    "# Since answers were cached on disk, we can simply re-query the model to load what we generated by running bake()\n",
    "# This was adapted from the notebook submission code where needed.\n",
    "def create_bakeoff_submission_mgb():\n",
    "    \"\"\" \"\n",
    "    The argument `model` is a `dspy.Module`. The return value of its\n",
    "    `forward` method must have an `answer` attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.join(\"data\", \"openqa\", \"cs224u-openqa-test-unlabeled.txt\")\n",
    "\n",
    "    # This should become a mapping from questions (str) to response\n",
    "    # dicts from your system.\n",
    "    gens = {}\n",
    "\n",
    "    with open(filename) as f:\n",
    "        questions = f.read().splitlines()\n",
    "\n",
    "    # Here we loop over the questions, run the system `model`, and\n",
    "    # store its `answer` value as the prediction:\n",
    "    for question in questions:\n",
    "        gens[question] = do_bake(question=question)\n",
    "\n",
    "    # Quick tests we advise you to run:\n",
    "    # 1. Make sure `gens` is a dict with the questions as the keys:\n",
    "    assert all(q in gens for q in questions)\n",
    "    # 2. Make sure the values are str:\n",
    "    assert all(isinstance(d, str) for d in gens.values())\n",
    "\n",
    "    # And finally the output file:\n",
    "    with open(\"cs224u-openqa-bakeoff-entry.json\", \"wt\") as f:\n",
    "        json.dump(gens, f, indent=4)\n",
    "\n",
    "\n",
    "create_bakeoff_submission_mgb()\n",
    "\n",
    "\n",
    "# STOP COMMENT: Please do not remove this comment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b39c60-7494-46a6-b450-42b7e9fe3aad",
   "metadata": {},
   "source": [
    "## Question 5: Bakeoff entry [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff871c1-cc38-4e2f-af38-45b3619e8329",
   "metadata": {},
   "source": [
    "For the bake-off, you simply need to be able to run your system on the file \n",
    "\n",
    "```data/openqa/cs224u-openqa-test-unlabeled.txt```\n",
    "\n",
    "The following code should download it for you if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4ca87f81-556b-46eb-904f-a3df70fdacb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-25T21:50:48.222086Z",
     "start_time": "2024-02-25T21:50:47.633628Z"
    }
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "if not os.path.exists(os.path.join(\"data\", \"openqa\", \"cs224u-openqa-test-unlabeled.txt\")):\n",
    "    os.makedirs(os.path.join('data', 'openqa'), exist_ok=True)\n",
    "    wget.download('https://web.stanford.edu/class/cs224u/data/cs224u-openqa-test-unlabeled.txt', out='data/openqa/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0024b-9af7-4e3b-930e-1e7603d4d85c",
   "metadata": {},
   "source": [
    "If the above fails, you can just download https://web.stanford.edu/class/cs224u/data/cs224u-openqa-test-unlabeled.txt and place it in `data/openqa`.\n",
    "\n",
    "This file contains only questions. The starter code below will help you structure this. It writes a file \"cs224u-openqa-bakeoff-entry.json\" to the current directory. That file should be uploaded as-is. Please do not change its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b0000-5bc0-4657-91e4-5a6e87f2f899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "\n",
    "def create_bakeoff_submission(model):\n",
    "    \"\"\"\"\n",
    "    The argument `model` is a `dspy.Module`. The return value of its\n",
    "    `forward` method must have an `answer` attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.join(\"data\", \"openqa\", \"cs224u-openqa-test-unlabeled.txt\")\n",
    "\n",
    "    # This should become a mapping from questions (str) to response\n",
    "    # dicts from your system.\n",
    "    gens = {}\n",
    "\n",
    "    with open(filename) as f:\n",
    "        questions = f.read().splitlines()\n",
    "\n",
    "    # Here we loop over the questions, run the system `model`, and\n",
    "    # store its `answer` value as the prediction:\n",
    "    for question in tqdm.tqdm(questions):\n",
    "        gens[question] = model(question=question).answer\n",
    "\n",
    "    # Quick tests we advise you to run:\n",
    "    # 1. Make sure `gens` is a dict with the questions as the keys:\n",
    "    assert all(question in gens for q in questions)\n",
    "    # 2. Make sure the values are str:\n",
    "    assert all(isinstance(d, str) for d in gens.values())\n",
    "\n",
    "    # And finally the output file:\n",
    "    with open(\"cs224u-openqa-bakeoff-entry.json\", \"wt\") as f:\n",
    "        json.dump(gens, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f32a8-547e-4a2c-8283-44adf69657ed",
   "metadata": {},
   "source": [
    "Here's what it looks like to evaluate our first program, `basic_qa_model`, on the bakeoff data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20e9ae-bb82-4dff-896f-aad7f150177d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create_bakeoff_submission(basic_qa_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
